<!DOCTYPE html>
<!-- saved from url=(0044)http://herohuyongtao.github.io/publications/ -->
<html class="js no-flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths gr_chenweikai_github_io" lang="en"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	

	<title>Weikai Chen | Research</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="./publications/modernizr-2.5.3.min.js.download"></script>

	<script src="./publications/jquery.min.js.download"></script>
	<script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.7.2.min.js"><\/script>')</script>

	<script src="./publications/spamspan.min.js.download"></script>
	<script src="./publications/prettify.js.download"></script>

	<link rel="stylesheet" href="./files/social_widget.css">
	<link rel="stylesheet" href="./files/glyphicons.css">
	<link rel="stylesheet" href="./files/bootstrap.css">
	<link rel="stylesheet" href="./files/bootstrap-responsive.css">
	<link rel="stylesheet" href="./files/app.css">
	<script type="text/javascript" src="./files/plugins.js.download"></script>
	<script type="text/javascript" src="./files/main.js.download"></script>
	<link rel="canonical" href="http://chenweikai.github.io/">

	<!-- to toggle text -->
	<style type="text/css">
		a.toggle_text_link {
			cursor:pointer;
		}

		pre.invisible_text {
			display: none;
		}
	</style>
	<script language="javascript" type="text/javascript">
		function toggle(element) {
			if(element.style.display=="block") {
				element.style.display="none";
			} else {
				element.style.display="block";
			}
		}
	</script>
</head>

<body class="page page-id-25 page-parent page-template-default top-navbar" data-gr-c-s-loaded="true">
  <!--[if lt IE 7]><div class="alert">Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</div><![endif]-->

    <header id="banner" class="navbar navbar-fixed-top" role="banner">
		<div class="navbar-inner">
			<div class="container">
				<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</a>
				<a class="brand" href="http://chenweikai.github.io/">
					Weikai Chen
				</a>
				<nav id="nav-main" class="nav-collapse" role="navigation">
					<ul class="nav">
						<li class="menu-home"><a href="http://chenweikai.github.io/">Home</a></li>
						<li class="menu-publications active active"><a href="http://chenweikai.github.io/publication.html">Publications</a></li>
						<!---<li class="menu-cv"><a href="http://chenweikai.github.io/cv/">CV</a></li>-->
					</ul>
				</nav>
			</div>
		</div>
	</header>

    <div id="wrap" class="container" role="document">
		<div id="content" class="row">
			<!-- publications div -->
			<div id="main" class="span12" role="main">
				<div class="page-header">
					<h1>Publications</h1>
				</div>
			</div>	
				
					
				</div>
				<!-- 2019 publications --> 
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2019</h2>
					</div>
				</div>
				
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/body19.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>SiCloPe: Silhouette-Based Clothed People</strong>
						</a>
						<br>
						Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li, Shigeo Morishima
						<br>
						<em> <b>CVPR 2019 (Oral Presentation)</b></em>
						<br>
						<em>"single-view based clothed human reconstruction"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="https://arxiv.org/pdf/1901.00049.pdf">paper</a></li>
							<!--<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(singleViewBody);">abstract</a>
								<pre id="singleViewBody" class="invisible_text">We introduce a new silhouette-based representation for modeling clothed human bodies using deep generative models. Our method can reconstruct a complete and textured 3D model of a person wearing clothes from a single input picture. Inspired by the visual hull algorithm, our implicit representation uses 2D silhouettes and 3D joints of a body pose to describe the immense shape complexity and variations of clothed people. Given a segmented 2D silhouette of a person and its inferred 3D joints from the input picture, we first synthesize consistent silhouettes from novel view points around the subject. The synthesized silhouettes, which are the most consistent with the input segmentation are fed into a deep visual hull algorithm for robust 3D shape prediction. We then infer the texture of the subject's back view using the frontal image and segmentation mask as input to a conditional generative adversarial network. Our experiments demonstrate that our silhouette-based model is an effective representation and the appearance of the back view can be predicted reliably using an image-to-image translation network. While classic methods based on parametric models often fail for single-view images of subjects with challenging clothing, our approach can still produce successful results, which are comparable to those obtained from multi-view input. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!-- <li><em class="icon-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li> -->
						</ul>
					</div>
				</div>
				
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/singleViewRecon2.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Soft Rasterizer: Differentiable Rendering for Unsupervised Single-View Mesh Reconstruction</strong>
						</a>
						<br>
						Shichen Liu, Weikai Chen, Tianye Li, Hao Li
						<br>
						<em> <b>arXiv 2019</b></em>
						<br>
						<em>"differentiable renderer for unsupervised single-view mesh reconstruction"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="https://arxiv.org/pdf/1901.05567.pdf">paper</a></li>
							<!--<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=xjTVECIqZfc&feature=youtu.be">video</a></li> -->
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(singleViewMesh);">abstract</a>
								<pre id="singleViewMesh" class="invisible_text">Rendering is the process of generating 2D images from 3D assets, simulated in a virtual environment, typically with a graphics pipeline. By inverting such renderer, one can think of a learning approach to predict a 3D shape from an input image. However, standard rendering pipelines involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence suitable for learning. We present the first non-parametric and truly differentiable rasterizer based on silhouettes. Our method enables unsupervised learning for high-quality 3D mesh reconstruction from a single image. We call our framework `soft rasterizer' as it provides an accurate soft approximation of the standard rasterizer. The key idea is to fuse the probabilistic contributions of all mesh triangles with respect to the rendered pixels. When combined with a mesh generator in a deep neural network, our soft rasterizer is able to generate an approximated silhouette of the generated polygon mesh in the forward pass. The rendering loss is back-propagated to supervise the mesh generation without the need of 3D training data. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art unsupervised techniques, both quantitatively and qualitatively. We also show that our soft rasterizer can achieve comparable results to the cutting-edge supervised learning method and in various cases even better ones, especially for real-world data. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!-- <li><em class="icon-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li> -->
						</ul>
					</div>
				</div>
				
				<!-- 2018 publications --> 
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2018</h2>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/eccv18body.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Deep Volumetric Video From Very Sparse Multi-View Performance Capture</strong>
						</a>
						<br>
						Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing, Chloe LeGendre, Linjie Luo, Chongyang Ma, Hao Li
						<br>
						<em><b>ECCV 2018</b></em>
						<br>
						<em>"volumetric body reconstruction from highly sparse views"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./papers/[ECCV18]Deep Volumetric Video From Very Sparse Multi-View Performance Capture.pdf">paper</a></li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=sQnXlQ3GyKc">video</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(zengBody);">abstract</a>
								<pre id="zengBody" class="invisible_text">We present a deep learning based volumetric approach for performance capture using a passive and highly sparse multi-view capture system. State-of-the-art performance capture systems require either pre-scanned actors, large number of cameras or active sensors. In this work, we focus on the task of template-free, per-frame 3D surface reconstruction from as few as three RGB sensors, for which conventional visual hull or multi-view stereo methods fail to generate plausible results.We introduce a novel multi-view Convolutional Neural Network (CNN) that maps 2D images to a 3D volumetric field and we use this field to encode the probabilistic distribution of surface points of the captured subject. By querying the resulting field, we can instantiate the clothed human body at arbitrary resolutions. Our approach scales to different numbers of input images, which yield increased reconstruction quality when more views are used. Although only trained on synthetic data, our network can generalize to handle real footage from body performance capture. Our method is suitable for high-quality low-cost full body volumetric capture solutions, which are gaining popularity for VR and AR content creation. Experimental results demonstrate that our method is significantly more robust and accurate than existing techniques when only very sparse views are available. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-file"></em> <a href="./papers/[ECCV18]body supplementary.pdf">suppl.</a></li>
						</ul>
					</div>
				</div>


				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/eccv18hair.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>HairNet: Single-View Hair Reconstruction using Convolutional Neural Networks</strong>
						</a>
						<br>
						Yi Zhou, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung, Xin Tong, Hao Li
						<br>
						<em><b>ECCV 2018</b></em>
						<br>
						<em>"deep learning based 3D hair reconstruction from a single image"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./papers/[ECCV18]HairNet - Single-View Hair Reconstruction using CNN.pdf">paper</a></li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=MLnS-gTWc9w">video</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(yi18hair);">abstract</a>
								<pre id="yi18hair" class="invisible_text">We introduce a deep learning-based method to generate full 3D hair geometry from an unconstrained image. Our method can recover local strand details and has real-time performance. State-of-the-art hair modeling techniques rely on large hairstyle collections for nearest neighbor retrieval and then perform ad-hoc refinement. Our deep learning approach, in contrast, is highly efficient in storage and can run 1000 times faster while generating hair with 30K strands. The convolutional neural network takes the 2D orientation field of a hair image as input and generates strand features that are evenly distributed on the parameterized 2D scalp. We introduce a collision loss to synthesize more plausible hairstyles, and the visibility of each strand is also used as a weight term to improve the reconstruction accuracy. The encoder-decoder architecture of our network naturally provides a compact and continuous representation for hairstyles, which allows us to interpolate naturally between hairstyles. We use a large set of rendered synthetic hair models to train our network. Our method scales to real images because an intermediate 2D orientation field, automatically calculated from the real image, factors out the difference between synthetic and real hairs. We demonstrate the effectiveness and robustness of our method on a wide range of challenging real Internet pictures and show reconstructed hair sequences from videos. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-file"></em> <a href="https://www.technologyreview.com/s/611569/the-best-of-the-physics-arxiv-week-ending-june-30-2018/">MIT Tech Review</a></li>
							<li><em class="icon-file"></em> <a href="https://news.developer.nvidia.com/ai-can-render-hair-in-3d-in-real-time/">Nvidia Review</a></li>
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/sig18face.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>High-Fidelity Facial Reflectance and Geometry Inference From an Unconstrained Image</strong>
						</a>
						<br>
						Shugo Yamaguchi*, Shunsuke Saito*, Koki Nagano, Yajie Zhao, Weikai Chen, Shigeo Morishima, Hao Li
						<br>
						<em><b>SIGGRAPH 2018</b></em>
						<br>
						<em>"inference of complete face reflectance maps from a single unconstrained image"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./papers/[SIG18]HFFRGIFUI.pdf">paper</a></li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=khNWYKfZwjQ">video</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(shugo18face);">abstract</a>
								<pre id="shugo18face" class="invisible_text">We present a deep learning-based technique to infer high-quality facial reflectance and geometry given a single unconstrained image of the subject, which may contain partial occlusions and arbitrary illumination conditions. The reconstructed high-resolution textures, which are generated in only a few seconds, include high-resolution skin surface reflectance maps, representing both the diffuse and specular albedo, and medium- and highfrequency displacement maps, thereby allowing us to render compelling digital avatars under novel lighting conditions. To extract this data, we train our deep neural networks with a high-quality skin reflectance and geometry database created with a state-of-the-art multi-view photometric stereo system using polarized gradient illumination. Given the raw facial texture map extracted from the input image, our neural networks synthesize complete reflectance and displacement maps, as well as complete missing regions caused by occlusions. The completed textures exhibit consistent quality throughout the face due to our network architecture, which propagates texture features from the visible region, resulting in high-fidelity details that are consistent with those seen in visible regions. We describe how this highly underconstrained problem is made tractable by dividing the full inference into smaller tasks, which are addressed by dedicated neural networks. We demonstrate the effectiveness of our network design with robust texture completion from images of faces that are largely occluded. With the inferred reflectance and geometry data, we demonstrate the rendering of high-fidelity 3D avatars from a variety of subjects captured under different lighting conditions. In addition, we perform evaluations demonstrating that our method can infer plausible facial reflectance and geometric details comparable to those obtained from high-end capture devices, and outperform alternative approaches that require only a single unconstrained input image.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-file"></em> <a href="./papers/[SIG18]suppl.pdf">suppl.</a></li>
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/mesoscopic.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Mesoscopic Facial Geometry inference Using Deep Neural Networks</strong>
						</a>
						<br>
						Loc  Huynh, Weikai Chen, Shunsuke Saito, Jun Xing, Koki Nagano, Andrew Jones, Hao Li, Paul Debevec
						<br>
						<em><b>CVPR 2018 (Spotlight Presentation)</b></em>
						<br>
						<em>"pore-level facial geometry inference from a single image"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./papers/[CVPR18]Mesoscopic_Facial_Geometry.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(loc18face);">abstract</a>
								<pre id="loc18face" class="invisible_text">We present a learning-based approach for synthesizing facial geometry at medium and fine scales from diffusely-lit facial texture maps. When applied to an image sequence, the synthesized detail is temporally coherent. Unlike current state-of-the-art methods [17, 5], which assume ”dark is deep”, our model is trained with measured facial detail collected using polarized gradient illumination in a Light Stage [20]. This enables us to produce plausible facial detail across the entire face, including where previous approaches may incorrectly interpret dark features as concavities such as at moles, hair stubble, and occluded pores. Instead of directly inferring 3D geometry, we propose to encode fine details in high-resolution displacement maps which are learned through a hybrid network adopting the state-of-the-art image-to-image translation network [29] and super resolution network [43]. To effectively capture geometric detail at both mid- and high frequencies, we factorize the learning into two separate sub-networks, enabling the full range of facial detail to be modeled. Results from our learning-based approach compare favorably with a high-quality active facial scanhening technique, and require only a single passive lighting condition without a complex scanning setup. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/impainting_.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Identity Preserving Face Completion for Large Ocular Region Occlusion</strong>
						</a>
						<br>
						Yajie Zhao, Weikai Chen, Jun Xing, Xiaoming Li, Zach Bessinger, Fuchang Liu, Wangmeng Zuo, Ruigang Yang
						<br>
						<em><b>BMVC 2018</b></em>
						<br>
						<em>"identity-preserved face inpainting for large occlusions"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="https://arxiv.org/abs/1807.08772">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(zhao18face);">abstract</a>
								<pre id="zhao18face" class="invisible_text">We present a novel deep learning approach to synthesize complete face images in the presence of large ocular region occlusions. This is motivated by recent surge of VR/AR displays that hinder face-to-face communications. Different from the state-of-the-art face inpainting methods that have no control over the synthesized content and can only handle frontal face pose, our approach can faithfully recover the missing content under various head poses while preserving the identity. At the core of our method is a novel generative etwork with dedicated constraints to regularize the synthesis process. To preserve the identity, our network takes an arbitrary occlusion-free image of the target identity to infer the missing content, and its high-level CNN features as an identity prior to regularize the searching space of generator. Since the input reference image may have a different pose, a pose map and a novel pose discriminator are further adopted to supervise the learning of implicit pose transformations. Our method is capable of generating coherent facial inpainting with consistent identity over videos with large variations of head motions. Experiments on both synthesized and real data demonstrate that our method greatly outperforms the state-of-the-art methods in terms of both synthesis quality and robustness. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/rbfnet.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions</strong>
						</a>
						<br>
						Weikai Chen, Xiaoguang Han, Guanbin Li, Chao Chen, Jun Xing, Yajie Zhao, Hao Li
						<br>
						<em><b>arXiv 2018</b></em>
						<br>
						<em>"deep point cloud feature based on radial basis functions"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="https://arxiv.org/pdf/1812.04302.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(zhao18face);">abstract</a>
								<pre id="zhao18face" class="invisible_text">Three-dimensional object recognition has recently achieved great progress thanks to the development of effective point cloud-based learning frameworks, such as PointNet and its extensions. However, existing methods rely heavily on fully connected layers, which introduce a significant amount of parameters, making the network harder to train and prone to overfitting problems. In this paper, we propose a simple yet effective framework for point set feature learning by leveraging a nonlinear activation layer encoded by Radial Basis Function (RBF) kernels. Unlike PointNet variants, that fail to recognize local point patterns, our approach explicitly models the spatial distribution of point clouds by aggregating features from sparsely distributed RBF kernels. A typical RBF kernel, e.g. Gaussian function, naturally penalizes long-distance response and is only activated by neighboring points. Such localized response generates highly discriminative features given different point distributions. In addition, our framework allows the joint optimization of kernel distribution and its receptive field, automatically evolving kernel configurations in an end-to-end manner. We demonstrate that the proposed network with a single RBF layer can outperform the state-of-the-art Pointnet++ in terms of classification accuracy for 3D object recognition tasks. Moreover, the introduction of nonlinear mappings significantly reduces the number of network parameters and computational cost, enabling significantly faster training and a deployable point cloud recognition solution on portable devices with limited resources. </pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<!-- 2017 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2017</h2>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/siga17decor.jpg" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="./projects/proj_siga17.html">
							<strong>Fabricable Tile Decors</strong>
						</a>
						<br>
						Weikai Chen, Yuexin Ma, Sylvain Lefebvre, Shiqing Xin, Jonàs Martínez and Wenping Wang
						<br>
						<em><b>SIGGRAPH Asia 2017</b></em>
						<br>
						<em>"a *magic* way to fabricate curved surfaces flatly"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./projects/proj_siga17.html">project page</a></li>
							<li><em class="icon-file"></em> <a href="./papers/weikai_siga17_tile.pdf">paper</a></li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=WgXxjnD9BA4">video</a></li>		
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(weikaiDecor);">abstract</a>
								<pre id="weikaiDecor" class="invisible_text">Recent advances in 3D printing have made it easier to manufacture customized objects by ordinary users in an aﬀordable manner, and therefore spurred high demand for more accessible methods for designing and fabricating 3D objects of various shapes and functionalities. In this paper we present a novel approach to model and fabricate surface-like objects composed of connected tiles, which can be used as objects in daily life, such as ornaments, covers, shades or handbags. Our method is designed to maximize the efciency and ease of fabrication. Given a base surface and a set of tile elements as user input, our method generates a tight packing of connected tiles on the surface. We apply an efcient and tailored optimization scheme to pack the tiles on the base surface with fabrication constraints. Then, to facilitate the fabrication process, we use a novel method based on minimal spanning tree to decompose the set of connected tiles into several connected patches. Each patch is articulated and can be developed into a plane. This allows printing with an inexpensive FDM printing process without requiring any supporting structures, which are often troublesome to remove. Finally, the separately printed patches are reassembled to form the fnal physical object, a shell surface composed of connected user-specifed tiles that take the shape of the input base surface. We demonstrate the utility of our method by modeling and fabricating a variety of objects, from simple decorative spheres to moderately complex surfaces, such as a handbag and a teddy bear. Several user controls are available, to distribute diﬀerent type of tiles over the surface and locally change their scales and orientations</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/tensor.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Tensor Field Design in Volumes</strong>
						</a>
						<br>
						Jonathan Palacios, Lawrence Roy, Prashant Kumar, Chen-Yuan Hsu, Weikai Chen, Chongyang Ma, Li-Yi Wei, Eugene Zhang
						<br>
						<em><b>SIGGRAPH Asia 2017</b></em>
						<br>
						<em>"the first framework to design and edit 3D tensors"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="http://www.chongyangma.com/publications/tf/index.html">project page</a></li>
							<li><em class="icon-file"></em> <a href="./papers/2017_tf_preprint.pdf">paper</a></li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=WgXxjnD9BA4">video</a></li>		
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(siga17tensor);">abstract</a>
								<pre id="siga17tensor" class="invisible_text">3D tensor field design is important in several graphics applications such as procedural noise, solid texturing, and geometry synthesis. Different fields can lead to different visual effects. The topology of a tensor !eld, such as degenerate tensors, can cause artifacts in these applications. Existing 2D tensor field design systems cannot be used to handle the topology of a 3D tensor field. In this paper, we present to our knowledge the first 3D tensor !eld design system. At the core of our system is the ability to edit the topology of tensor !elds. We demonstrate the power of our design system with applications in solid texturing and geometry synthesis.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/porous.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>By Example Synthesis of Three-Dimensional Porous Materials</strong>
						</a>
						<br>
						Hui Zhang, Weikai Chen, Bin Wang and Wenping Wang
						<br>
						<em><b>GMP 2017</b></em>
						<br>
						<em>"example-based 3D porous structure synthesis"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./papers/[GMP17]By Example Synthesis of Three-Dimensional Porous Materials.pdf">paper</a></li>		
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(zhang17porous);">abstract</a>
								<pre id="zhang17porous" class="invisible_text">Porous materials are ubiquitous in nature and are used for many applications. However, there is still a lack of computational methods for generating and modeling complex porous structures. While conventional texture synthesis methods succeed in synthesizing solid texture based on a 2D input, to generate a 3D structure that visually matches a given 3D exemplar remains an open question. We present the first framework that can synthesize porous material that is structurally consistent to input 3D exemplar. In our framework, the 2D texture optimization method is extended built upon 3D neighborhood. An adaptive weighted mechanism method is proposed to reduce blurring and accelerate the convergence speed. Moreover, a connectivity pruning algorithm is performed as post-processing to prune spurious branches. Experimental results demonstrate that our method can preserve both the structural continuity and material descriptors of input exemplar while maintain visual similarity with input structure.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>


				<!-- 2016 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2016</h2>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/chen_sig16.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="./projects/proj_sig16.html">
							<strong>Synthesis of Filigrees for Digital Fabrication</strong>
						</a>
						<br>
						Weikai Chen, Xiaolong Zhang, <a href="https://sites.google.com/site/xinshiqing/">Shiqing Xin</a>, <a href="http://xiayang.weebly.com/">Yang Xia </a>, <a href="http://www.antexel.com/sylefeb/research">Sylvain Lefebvre</a> and <a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br>
						<em><b>SIGGRAPH 2016</b></em>
						<br>
						<em>"how to make example-based 3D jewelry"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./projects/proj_sig16.html">project page</a></li>
							<li><em class="icon-file"></em> <a href="./papers/weikai_sig16_filigree_final.pdf">paper</a></li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=clzlyXEzoNw">video</a></li>		
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(weikai16filigree);">abstract</a>
								<pre id="weikai16filigree" class="invisible_text">Filigrees are thin patterns found in jewelry, ornaments and lace fabrics. They are often formed of repeated base elements manually composed into larger, delicate patterns. Digital fabrication simplifies the process of turning a virtual model of a filigree into a physical object. However, designing a virtual model of a filigree remains a time consuming and challenging task. The difficulty lies in tightly packing together the base elements while covering a target surface. In addition, the filigree has to be well connected and sufficiently robust to be fabricated. We propose a novel approach automating this task. Our technique covers a target surface with a set of input base elements, forming a filigree strong enough to be fabricated. We exploit two properties of filigrees to make this possible. First, as filigrees form delicate traceries they are well captured by their skeleton. This affords for a simpler definition of operators such as matching and deformation. Second, instead of seeking for a perfect packing of the base elements we relax the problem by allowing appearance preserving partial overlaps. We optimize a filigree by a stochastic search, further improved by a novel boosting algorithm that records and reuses good configurations discovered during the process. We illustrate our technique on a number of challenging examples reproducing filigrees on large objects, which we manufacture by 3D printing. Our technique affords for several user controls, such as the scale and orientation of the elements.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-file"></em> <a href="https://www.youtube.com/watch?v=cVZzkSaxKmY">two-minute paper</a></li>
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/leaf_Torus_maj.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Tensor Field Design in Volumes</strong>
						</a>
						<br>
						Jonathan Palacios, Chongyang Ma, Weikai Chen, Li-Yi Wei and Eugene Zhang
						<br>
						<em><b>SIGGRAPH Asia 2016</b> Technical Briefs</em>
						<br>
						<em>"a lightweight 3D tensor designing and editing system"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="http://www.liyiwei.org/papers/texture-siga16/">project page</a></li>
							<li><em class="icon-file"></em> <a href="./papers/[SIGA16]Tensor_field_design.pdf">paper</a></li>	
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(Jonathan16brief);">abstract</a>
								<pre id="Jonathan16brief" class="invisible_text">The design of 3D tensor fields is important in several graphics applications such as procedural noise, solid texturing, and geometry synthesis. Different fields can lead to different visual effects. The topology of a tensor field, such as degenerate tensors, can cause artifacts in these applications. Existing 2D tensor field design systems cannot handle the topology of 3D tensor fields. We present, to our best knowledge, the first 3D tensor field design system. At the core of our system is the ability to specify and control the type, number, location, shape, and connectivity of degenerate tensors. To enable such capability, we have made a number of observations of tensor field topology that were previously unreported. We demonstrate applications of our method in volumetric synthesis of solid and geometry texture as well as anisotropic Gabor noise.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<!-- previous publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2012</h2>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/signal.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>Second-order differential based matching pursuit method for compressive sensing signal recovery</strong>
						</a>
						<br>
						Weikai Chen, Yunhui Chen
						<br>
						<em><b>WCSP 2012</b> </em>
						<br>
						<em>"all about signal compression"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="http://ieeexplore.ieee.org/document/6542991/">paper</a></li>	
						</ul>
					</div>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="http://chenweikai.github.io/publication.html">
							<img class="dropshadow pull-right" alt="" src="./figures/snr.png" width="150" height="115">
						</a>
					</div>
					<div class="span8">
						<a href="http://chenweikai.github.io/publication.html">
							<strong>A Compressive Sensing Method for Estimating Doubly-Selective Sparse Channels in OFDM Systems</strong>
						</a>
						<br>
						Kaihua Liu, <b>Weikai Chen (corresponding author)</b>, Yongtao Ma
						<br>
						<em>Journal of Tianjin University </em>
						<br>
						<em>"channel estimation for OFDM system"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="http://chenweikai.github.io/publication.html">paper</a></li>	
						</ul>
					</div>
				</div>

				<!-- thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>Ph.D. Dissertation</h2>
					</div>
				</div>

				<!-- phd thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="./figures/hku.png" width="150" height="90">
					</div>
					<div class="span8">
						<a href="http://herohuyongtao.github.io/publications/">
							<strong>Synthesizing patterned surfaces for 3D printing</strong>
						</a>
						<br>
						Weikai Chen
						<br>
						<em>The University of Hong Kong, 2017</em>
						<br>
						<em>"basically a concatenation of my first two SIGGRAPH/SIGGRAPH Asia papers"</em>
						<br>
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="./papers/phd_thesis.pdf">thesis</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(chen2017thesis);">abstract</a>
								<pre id="chen2017thesis" class="invisible_text">Recent years have witnessed the advancement of 3D printing in fabricating objects with sophisticated and highly-customized geometries. The print services are now widely available through online orders, home printers and local FabLabs. Nevertheless, it remains difficult for most users to create interesting objects, even more so when the intended design has complex geometry details. To circumvent this issue, in this thesis we present approaches to automate the task of designing and fabricating artistic patterned surfaces. 
								<br>We firstly present a novel approach to synthesize fabricable filigrees over target surfaces. As thin patterns widely found in jewelry, ornaments and lace fabrics, filigrees are often manually designed by composing repeated base elements. Our technique aims to automate this challenging task. Our technique covers a target surface with a set of input base elements, forming a filigree strong enough to be fabricated. We leverage the fact that as traceries, filigrees can be well captured by their skeletons. This affords for novel energy function that measures the matching quality between base elements. In addition, instead of seeking for a perfect packing of base elements, we relax the problem by allowing appearance-preserving partial overlaps. The formulation is optimized by a stochastic search, which is further improved by a boosting step that records and reuses good configurations discovered during process. Our technique affords for multi-class synthesis and several user controls, such as scale and orientation of the elements. 
								<br>Second, we extend the method to generate complex – yet easy to print -- tile decorations. The user only provides base surface and a set of tiles. Our algorithm automatically decorates the base surface with the tiles. However, rather than being simple decals, the tiles \textit{become} the final object, producing shell-like surfaces that can be used as ornaments, covers, shades and even handbags. Our technique is designed to maximize print efficiency: the results are printed as independent flat patches that are articulated sets of tiles. The patches could be assembled into the final surface through the use of snap-fit connectors. Our approach proceeds in three steps. First, a dedicated packing algorithm is proposed to compute a tile layout while taking into account fabrication constraints, in particular ensuring hinges can be inserted between neighboring tiles. A second step extracts the patches to be printed and folded, while the third step optimizes the location of snap-fit connectors. Our technique works on a variety of objects, from simple decorative spheres to moderately complex shapes.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(chen2017thesisbib);">bibtex</a>
								<pre id="chen2017thesisbib" class="invisible_text">@phdthesis{chen2017synthesizing,
title={Synthesizing patterned surfaces for 3D printing},
author={Chen, Weikai},
journal={HKU Theses Online (HKUTO)},
year={2017},
publisher={The University of Hong Kong (Pokfulam, Hong Kong)}
}
</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>
			</div><!-- /#main -->
		</div><!-- /#content -->
    </div><!-- /#wrap -->

	<footer id="content-info" class="container" role="contentinfo">
		<table width="100%">
			<tbody><tr>
				<td>
					<p class="copy"><small>© 2018 Weikai Chen</small></p>
				</td>
				<td style="text-align:center">
					<p class="copy"><small>Last updated on 10/2018</small></p>
				</td>
				<td style="text-align:right">
					<a href="http://sitestates.com/" title="Site access statistics">
						<img src="./figures/31185.jpg" border="0">
					</a>
				</td>
			</tr>
		</tbody></table>
	</footer>

	<!-- From http://stackoverflow.com/a/11668413/72470 -->
	<script>
	  !function ($) {
		$(function(){
		  window.prettyPrint && prettyPrint()
		})
	  }(window.jQuery)
	</script>



<div id="cntvlive2-is-installed"></div></body></html>